{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = r\"../dataset/x_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>added support set feature xmlpath use xmlpathconfig use example disable external dtd loading also shortcut use xmlpathconfigdisableloadingofexternaldtd method issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>improve performance type qualifier analysis shrink debug output type qualifier analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>duplicate code unify schedule merge job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fix issue ecj transformer yieldyield ifstatements break statemachine also add test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>add additional model abstraction allow progress button task exchange runtime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>jbide poor performance xhtml template validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>minor abstraction sparqlparserbase make easy extend subquery parsing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>remove dead code relate elasticsearch utility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>static utility class rename junit compatibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>improve typename inference mock throw work variable declaration also variable assignment mock framework improvement imockinvocation reveals mock type imockinvocation accessible result generator unofficial feature improve tostring output mock object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>426 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                              lemmatized_text\n",
       "0                                                                                        added support set feature xmlpath use xmlpathconfig use example disable external dtd loading also shortcut use xmlpathconfigdisableloadingofexternaldtd method issue\n",
       "1                                                                                                                                                                     improve performance type qualifier analysis shrink debug output type qualifier analysis\n",
       "2                                                                                                                                                                                                                     duplicate code unify schedule merge job\n",
       "3                                                                                                                                                                          fix issue ecj transformer yieldyield ifstatements break statemachine also add test\n",
       "4                                                                                                                                                                                add additional model abstraction allow progress button task exchange runtime\n",
       "..                                                                                                                                                                                                                                                        ...\n",
       "421                                                                                                                                                                                                          jbide poor performance xhtml template validation\n",
       "422                                                                                                                                                                                      minor abstraction sparqlparserbase make easy extend subquery parsing\n",
       "423                                                                                                                                                                                                             remove dead code relate elasticsearch utility\n",
       "424                                                                                                                                                                                                           static utility class rename junit compatibility\n",
       "425  improve typename inference mock throw work variable declaration also variable assignment mock framework improvement imockinvocation reveals mock type imockinvocation accessible result generator unofficial feature improve tostring output mock object\n",
       "\n",
       "[426 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = pd.read_csv(filePath, encoding='ISO-8859-1')\n",
    "\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = r\"../dataset/x_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>improve performance messagesource condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>full support xelemental injection htmltemplate inheritance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fix brokennullcheck also catch problem equality expression offend code rule also match array literal expression slightly great accuracy comment cleanups gitsvnid bafdacfced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sshd reame listfiles listsshfiles allow implementation use ftpserver file abstraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fix dimension properly reduce imagepluscontianers always break eg gaussian convolution handle dimension size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>core add random functionality filter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>add googreflectobjectproperty first class primitive functionality jscompilerrenameproperty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>continued cleanup make table join inheritance match nullablesizes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>dao abstraction update npc sql file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>fix miss validation error message value conversion fail eclipse bug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1275 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                   lemmatized_text\n",
       "0                                                                                                                                      improve performance messagesource condition\n",
       "1                                                                                                                       full support xelemental injection htmltemplate inheritance\n",
       "2     fix brokennullcheck also catch problem equality expression offend code rule also match array literal expression slightly great accuracy comment cleanups gitsvnid bafdacfced\n",
       "3                                                                                            sshd reame listfiles listsshfiles allow implementation use ftpserver file abstraction\n",
       "4                                                                     fix dimension properly reduce imagepluscontianers always break eg gaussian convolution handle dimension size\n",
       "...                                                                                                                                                                            ...\n",
       "1270                                                                                                                                          core add random functionality filter\n",
       "1271                                                                                    add googreflectobjectproperty first class primitive functionality jscompilerrenameproperty\n",
       "1272                                                                                                             continued cleanup make table join inheritance match nullablesizes\n",
       "1273                                                                                                                                           dao abstraction update npc sql file\n",
       "1274                                                                                                           fix miss validation error message value conversion fail eclipse bug\n",
       "\n",
       "[1275 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pd.read_csv(filePath, encoding='ISO-8859-1')\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = r\"../dataset/y_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>external</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>code smell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bugfix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>external</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>code smell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>external</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>426 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category\n",
       "0    functional\n",
       "1      external\n",
       "2    code smell\n",
       "3        bugfix\n",
       "4      internal\n",
       "..          ...\n",
       "421    external\n",
       "422    internal\n",
       "423  code smell\n",
       "424    external\n",
       "425  functional\n",
       "\n",
       "[426 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = pd.read_csv(filePath, encoding='ISO-8859-1')\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = r\"../dataset/y_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>external</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bugfix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>external</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>external</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>bugfix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1275 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category\n",
       "0     external\n",
       "1     internal\n",
       "2       bugfix\n",
       "3     internal\n",
       "4     internal\n",
       "...        ...\n",
       "1270  external\n",
       "1271  external\n",
       "1272  internal\n",
       "1273  internal\n",
       "1274    bugfix\n",
       "\n",
       "[1275 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.read_csv(filePath, encoding='ISO-8859-1')\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "\n",
    "# Fit and transform the training data\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Only transform the testing data\n",
    "x_test_tfidf = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit(self, X, y):\n",
    "        \n",
    "#         X_preprocessed = preprocess(X.copy())\n",
    "        \n",
    "#         # Separating text data for TF-IDF transformation\n",
    "#         text_data = X_preprocessed.pop('combined_text')\n",
    "        \n",
    "#         text_features = self.tfidf_vectorizer.fit_transform(text_data)\n",
    "        \n",
    "#         # Combining text features with other features\n",
    "#         X_combined = np.hstack((text_features.toarray(), X_preprocessed.values))\n",
    "        \n",
    "#         self.classifier.fit(X_combined, y)\n",
    "        \n",
    "#         # Define a broad range of parameters for RandomizedSearchCV\n",
    "#         rf_random_params = {\n",
    "#             'n_estimators': np.arange(100, 1001, 100),\n",
    "#             'max_depth': np.arange(10, 101, 10),\n",
    "#             'min_samples_split': np.arange(2, 11, 1),\n",
    "#             'criterion': ['gini', 'entropy']\n",
    "#         }\n",
    "        \n",
    "#         # Randomized Search with Cross-Validation\n",
    "#         self.rfc = RandomForestClassifier(class_weight=\"balanced\", random_state=1234)\n",
    "#         random_search = RandomizedSearchCV(self.rfc, rf_random_params, n_iter=100, cv=5, scoring='f1', n_jobs=-1, random_state=1234)\n",
    "#         random_search.fit(X_combined, y)\n",
    "#         print(\"Best parameters from RandomizedSearch: \", random_search.best_params_)\n",
    "\n",
    "#         # Refine search with GridSearchCV around the best parameters found\n",
    "#         best_params = random_search.best_params_\n",
    "#         rf_grid_params = {\n",
    "#             'n_estimators': [best_params['n_estimators'] - 50, best_params['n_estimators'], best_params['n_estimators'] + 50],\n",
    "#             'max_depth': [best_params['max_depth'] - 10, best_params['max_depth'], best_params['max_depth'] + 10],\n",
    "#             'min_samples_split': [best_params['min_samples_split'] - 1, best_params['min_samples_split'], best_params['min_samples_split'] + 1],\n",
    "#             'criterion': [best_params['criterion']]\n",
    "#         }\n",
    "#         self.rscv = GridSearchCV(self.rfc, rf_grid_params, cv=5, scoring='f1', n_jobs=-1)\n",
    "#         self.rscv.fit(X_combined, y)\n",
    "#         print(\"Refined best parameters from GridSearchCV: \", self.rscv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming x_train_tfidf and y_train are already defined as your training dataset and labels\n",
    "\n",
    "# # Initialize RandomForestClassifier\n",
    "# rf_classifier = RandomForestClassifier(class_weight=\"balanced\", random_state=43)\n",
    "\n",
    "# # Define the broad parameter space for RandomizedSearchCV\n",
    "# random_grid = {\n",
    "#     'n_estimators': np.arange(10, 1001, 50),  # Number of trees in the forest\n",
    "#     'max_depth': np.arange(10, 101, 10),  # Maximum depth of the tree\n",
    "#     'min_samples_split': np.arange(2, 11, 1),  # Minimum number of samples required to split an internal node\n",
    "#     'min_samples_leaf': np.arange(1, 11, 1),  # Minimum number of samples required to be at a leaf node\n",
    "#     'bootstrap': [True, False],  # Method of selecting samples for training each tree\n",
    "#     'criterion': ['gini', 'entropy'],  # The function to measure the quality of a split\n",
    "#     'max_features': ['auto', 'sqrt', 'log2', None],  # Number of features to consider at every split\n",
    "#     'min_impurity_decrease': np.linspace(0.0, 0.1, 10),  # Threshold for early stopping in tree growth\n",
    "#     'max_leaf_nodes': [None] + list(np.arange(10, 1000, 50))  # Maximum number of leaf nodes\n",
    "# }\n",
    "\n",
    "# # Random search of parameters, using 3 fold cross validation,\n",
    "# # search across 100 different combinations, and use all available cores\n",
    "# rf_random = RandomizedSearchCV(estimator=rf_classifier, param_distributions=random_grid, n_iter=100, cv=3, verbose=2, random_state=1234, n_jobs=-1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# rf_random.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# # Output the best parameters from RandomizedSearchCV\n",
    "# print(\"Best parameters found by RandomizedSearchCV:\")\n",
    "# print(rf_random.best_params_)\n",
    "\n",
    "# # You can now take the best parameters from the random search and use them to\n",
    "# # create a more focused search with GridSearchCV (if necessary). This might involve\n",
    "# # narrower ranges of parameters or specific combinations that you want to test exhaustively.\n",
    "\n",
    "# # Refine search with GridSearchCV around the best parameters found\n",
    "# best_params = rf_random.best_params_\n",
    "# rf_grid_params = {\n",
    "#     'n_estimators': [best_params.get('n_estimators', 100) - 50, best_params.get('n_estimators', 100), best_params.get('n_estimators', 100) + 50],\n",
    "#     'max_depth': [best_params.get('max_depth', 10) - 10, best_params.get('max_depth', 10), best_params.get('max_depth', 10) + 10],\n",
    "#     'min_samples_split': [max(2, best_params.get('min_samples_split', 2) - 1), best_params.get('min_samples_split', 2), best_params.get('min_samples_split', 2) + 1],\n",
    "#     'min_samples_leaf': [max(1, best_params.get('min_samples_leaf', 1) - 1), best_params.get('min_samples_leaf', 1), best_params.get('min_samples_leaf', 1) + 1],\n",
    "#     'bootstrap': [best_params['bootstrap']],\n",
    "#     'criterion': [best_params['criterion']],\n",
    "#     'max_features': [best_params['max_features']],\n",
    "#     'min_impurity_decrease': [max(0.0, best_params.get('min_impurity_decrease', 0.0) - 0.01), best_params.get('min_impurity_decrease', 0.0), best_params.get('min_impurity_decrease', 0.0) + 0.01],\n",
    "#     'max_leaf_nodes': [best_params.get('max_leaf_nodes', None)]\n",
    "# }\n",
    "# rscv = GridSearchCV(rf_classifier, rf_grid_params, cv=5, scoring='f1', n_jobs=-1)\n",
    "# rscv.fit(x_train_tfidf, y_train)\n",
    "# print(\"Refined best parameters from GridSearchCV: \", rscv.best_params_)\n",
    "\n",
    "\n",
    "# # {'n_estimators': 1000, 'min_samples_split': 8, 'max_depth': 90, 'criterion': 'gini', 'bootstrap': False}\n",
    "# # Refined best parameters from GridSearchCV:  {'criterion': 'gini', 'max_depth': 80, 'min_samples_split': 7, 'n_estimators': 950}\n",
    "\n",
    "# # {'n_estimators': 200, 'max_depth': 40, 'criterion': 'gini', 'bootstrap': False}\n",
    "# # Refined best parameters from GridSearchCV:  {'bootstrap': False, 'criterion': 'gini', 'max_depth': 30, 'n_estimators': 150}\n",
    "\n",
    "# # {'bootstrap': False, 'criterion': 'gini', 'max_depth': 80, 'n_estimators': 950}\n",
    "# # {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 41, 'n_estimators': 551}\n",
    "# # {'bootstrap': False, 'criterion': 'gini', 'max_depth': 41, 'n_estimators': 151}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.svm import SVC\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming x_train_tfidf and y_train are already defined\n",
    "\n",
    "# # Initialize the SVC model\n",
    "# svc = SVC(random_state=42)\n",
    "\n",
    "# # Define a parameter space for RandomizedSearchCV\n",
    "# param_distributions = {\n",
    "#     'C': np.logspace(-4, 4, 20),  # Regularization parameter\n",
    "#     'gamma': ['scale', 'auto'],  # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "#     'kernel': ['linear', 'poly', 'rbf', 'sigmoid']  # Specifies the kernel type to be used in the algorithm\n",
    "# }\n",
    "\n",
    "# # Random search of parameters, using 3 fold cross validation,\n",
    "# # search across a wide range of combinations, and use all available cores\n",
    "# svm_random = RandomizedSearchCV(estimator=svc, param_distributions=param_distributions, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# svm_random.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# # Output the best parameters from RandomizedSearchCV\n",
    "# print(\"Best parameters found by RandomizedSearchCV for SVM:\")\n",
    "# print(svm_random.best_params_)\n",
    "\n",
    "\n",
    "# # Extract the best parameters found by RandomizedSearchCV\n",
    "# best_params = svm_random.best_params_\n",
    "\n",
    "# # Create a parameter grid focused around the best parameters found\n",
    "# param_grid = {\n",
    "#     'C': [best_params['C'] * 0.5, best_params['C'], best_params['C'] * 2],\n",
    "#     'gamma': [best_params['gamma']],\n",
    "#     'kernel': [best_params['kernel']]\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV for a more focused search\n",
    "# svm_grid = GridSearchCV(estimator=SVC(random_state=42), param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# # Fit GridSearchCV\n",
    "# svm_grid.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# # Output the best parameters from GridSearchCV\n",
    "# print(\"Refined best parameters from GridSearchCV for SVM:\")\n",
    "# print(svm_grid.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
    "# # Best parameters found by RandomizedSearchCV for SVM:\n",
    "# # {'kernel': 'rbf', 'gamma': 0.001, 'C': 545.5594781168514}\n",
    "# # Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
    "# # Refined best parameters from GridSearchCV for SVM:\n",
    "# # {'C': 272.7797390584257, 'gamma': 0.002, 'kernel': 'rbf'}\n",
    "# # {'C': 5000.0, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "# # {'C': 14.881757208156566, 'gamma': 'auto', 'kernel': 'linear'\n",
    "# # {'C': 0.8118883695943605, 'gamma': 'auto', 'kernel': 'linear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming x_train_tfidf and y_train are defined\n",
    "\n",
    "# # Initialize DecisionTreeClassifier\n",
    "# dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # Define the parameter space for RandomizedSearchCV\n",
    "# param_distributions = {\n",
    "#     'max_depth': np.arange(10, 101, 10),  # Maximum depth of the tree\n",
    "#     'criterion': ['gini', 'entropy']  # The function to measure the quality of a split\n",
    "# }\n",
    "\n",
    "# # Random search of parameters, using 3 fold cross validation,\n",
    "# # search across a wide range of combinations, and use all available cores\n",
    "# dt_random = RandomizedSearchCV(estimator=dt_classifier, param_distributions=param_distributions, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# dt_random.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# # Output the best parameters from RandomizedSearchCV\n",
    "# print(\"Best parameters found by RandomizedSearchCV for Decision Tree:\")\n",
    "# print(dt_random.best_params_)\n",
    "\n",
    "# # Extract the best parameters found by RandomizedSearchCV\n",
    "# best_params = dt_random.best_params_\n",
    "\n",
    "# # Create a parameter grid focused around the best parameters found\n",
    "# param_grid = {\n",
    "#     'max_depth': [best_params['max_depth'] - 10, best_params['max_depth'], best_params['max_depth'] + 10] if best_params['max_depth'] is not None else np.arange(5, 16, 5),\n",
    "#     'criterion': [best_params['criterion']]\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV for a more focused search\n",
    "# dt_grid = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# # Fit GridSearchCV\n",
    "# dt_grid.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# # Output the best parameters from GridSearchCV\n",
    "# print(\"Refined best parameters from GridSearchCV for Decision Tree:\")\n",
    "# print(dt_grid.best_params_)\n",
    "\n",
    "# # {'criterion': 'gini', 'max_depth': 30}\n",
    "# # # {'criterion': 'gini', 'max_depth': 20\n",
    "# # {'criterion': 'gini', 'max_depth': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming x_train_tfidf and y_train are defined\n",
    "\n",
    "# # # Initialize LogisticRegression\n",
    "# # logistic_regression = LogisticRegression(random_state=42, max_iter=10000)\n",
    "\n",
    "# # # Define the parameter space for RandomizedSearchCV\n",
    "# param_distributions = {\n",
    "#     'C': np.logspace(-4, 4, 20),  # Regularization strength\n",
    "#     'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Type of regularization\n",
    "#     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']  # Algorithm to use in the optimization problem\n",
    "# }\n",
    "\n",
    "# # Random search of parameters, using 3 fold cross validation,\n",
    "# # search across a wide range of combinations, and use all available cores\n",
    "# lr_random = RandomizedSearchCV(estimator=logistic_regression, param_distributions=param_distributions, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# lr_random.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# # Output the best parameters from RandomizedSearchCV\n",
    "# print(\"Best parameters found by RandomizedSearchCV for Logistic Regression:\")\n",
    "# print(lr_random.best_params_)\n",
    "\n",
    "# # Extract the best parameters found by RandomizedSearchCV\n",
    "# best_params = lr_random.best_params_\n",
    "\n",
    "# # Create a parameter grid focused around the best parameters found\n",
    "# param_grid = {\n",
    "#     'C': [best_params['C'] * 0.5, best_params['C'], best_params['C'] * 2],\n",
    "#     'penalty': [best_params['penalty']] if best_params['penalty'] != 'elasticnet' else ['l1', 'l2'],\n",
    "#     'solver': ['liblinear', 'saga'] if best_params['penalty'] == 'l1' else ['newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "# }\n",
    "\n",
    "# # Some combinations might not be compatible, adjust the grid as needed\n",
    "# # Create a GridSearchCV for a more focused search\n",
    "# lr_grid = GridSearchCV(estimator=LogisticRegression(random_state=42, max_iter=10000), param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# # Fit GridSearchCV\n",
    "# lr_grid.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# # Output the best parameters from GridSearchCV\n",
    "# print(\"Refined best parameters from GridSearchCV for Logistic Regression:\")\n",
    "# print(lr_grid.best_params_)\n",
    "\n",
    "# # {'C': 2.140666199359698, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
    "# # {'C': 3.247553478377442, 'penalty': 'l1', 'solver': 'liblinear'\n",
    "# # {'C': 3.247553478377442, 'penalty': 'l1', 'solver': 'saga'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming x_train_tfidf and y_train are defined\n",
    "\n",
    "# # Initialize MultinomialNB\n",
    "# mnb = MultinomialNB()\n",
    "\n",
    "# # Define the parameter space for RandomizedSearchCV\n",
    "# param_distributions = {\n",
    "#     'alpha': np.linspace(0, 1, 10)  # Additive (Laplace/Lidstone) smoothing parameter\n",
    "# }\n",
    "\n",
    "# # Since it's just one parameter, we're technically not \"randomizing\" much here\n",
    "# mnb_random = RandomizedSearchCV(estimator=mnb, param_distributions=param_distributions, n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# mnb_random.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# # Output the best parameters from RandomizedSearchCV\n",
    "# print(\"Best parameters found by RandomizedSearchCV for Multinomial Naive Bayes:\")\n",
    "# print(mnb_random.best_params_)\n",
    "\n",
    "# # Extract the best alpha found by RandomizedSearchCV\n",
    "# best_alpha = mnb_random.best_params_['alpha']\n",
    "\n",
    "# # Create a parameter grid focused around the best alpha found\n",
    "# param_grid = {\n",
    "#     'alpha': [max(0, best_alpha - 0.1), best_alpha, best_alpha + 0.1]\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV for a more focused search\n",
    "# mnb_grid = GridSearchCV(estimator=MultinomialNB(), param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# # Fit GridSearchCV\n",
    "# mnb_grid.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# # Output the best parameters from GridSearchCV\n",
    "# print(\"Refined best parameters from GridSearchCV for Multinomial Naive Bayes:\")\n",
    "# print(mnb_grid.best_params_)\n",
    "\n",
    "# # {'alpha': 1.1}\n",
    "# # # {'alpha': 0.9\n",
    "# #{'alpha': 1.1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming x_train_tfidf and y_train are defined\n",
    "\n",
    "# # Initialize KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier()\n",
    "\n",
    "# # Define the parameter space for RandomizedSearchCV\n",
    "# param_distributions = {\n",
    "#     'n_neighbors': np.arange(1, 50),  # Number of neighbors to use\n",
    "#     'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
    "# }\n",
    "\n",
    "# # Random search of parameters, using 3 fold cross validation,\n",
    "# # search across a wide range of combinations, and use all available cores\n",
    "# knn_random = RandomizedSearchCV(estimator=knn, param_distributions=param_distributions, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# knn_random.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# # Output the best parameters from RandomizedSearchCV\n",
    "# print(\"Best parameters found by RandomizedSearchCV for KNN:\")\n",
    "# print(knn_random.best_params_)\n",
    "\n",
    "# # Extract the best parameters found by RandomizedSearchCV\n",
    "# best_params = knn_random.best_params_\n",
    "\n",
    "# # Create a parameter grid focused around the best parameters found\n",
    "# param_grid = {\n",
    "#     'n_neighbors': [best_params['n_neighbors'] - 2, best_params['n_neighbors'] - 1, best_params['n_neighbors'], best_params['n_neighbors'] + 1, best_params['n_neighbors'] + 2],\n",
    "#     'weights': [best_params['weights']],\n",
    "# }\n",
    "\n",
    "# # Ensure values are within valid ranges\n",
    "# param_grid['n_neighbors'] = [n for n in param_grid['n_neighbors'] if n > 0]\n",
    "\n",
    "# # Create a GridSearchCV for a more focused search\n",
    "# knn_grid = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# # Fit GridSearchCV\n",
    "# knn_grid.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# # Output the best parameters from GridSearchCV\n",
    "# print(\"Refined best parameters from GridSearchCV for KNN:\")\n",
    "# print(knn_grid.best_params_)\n",
    "\n",
    "# # {'n_neighbors': 43, 'weights': 'distance'}\n",
    "# # {'n_neighbors': 46, 'weights': 'distance'\n",
    "# # {'n_neighbors': 42, 'weights': 'distance'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # \"Random Forest\": RandomForestClassifier(max_depth=80, n_estimators=950, criterion='gini', bootstrap=False)\n",
    "# #     \"Support Vector Classification\": SVC(gamma='auto', kernel='rbf', C=5000.0),\n",
    "# #     \"Decision Tree\": DecisionTreeClassifier(max_depth=30, criterion='gini'),\n",
    "# #     \"Logistic Regression\": LogisticRegression(penalty='l2', C=2.14, solver='newton-cg'),\n",
    "# #     \"Multinomial Naive Bayes\": MultinomialNB(alpha=1.1),\n",
    "# #     \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=43, weights='distance')\n",
    "    \n",
    "# classifiers = {\n",
    "#     \"Random Forest\": RandomForestClassifier(bootstrap=True, criterion='gini', max_depth=None, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease= 0.0011111111111111113, min_samples_leaf= 1, min_samples_split= 3, n_estimators= 100),\n",
    "#     \"Support Vector Classification\": SVC(gamma='auto', kernel='linear', C=0.8119),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(max_depth=20, criterion='gini'),\n",
    "#     \"Logistic Regression\": LogisticRegression(penalty='l1', C=3.246, solver='saga'),\n",
    "#     \"Multinomial Naive Bayes\": MultinomialNB(alpha=1.1),\n",
    "#     \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=42, weights='distance')\n",
    "# }\n",
    "\n",
    "# # Train, predict, and evaluate each classifier separately\n",
    "# for classifier_name, classifier in classifiers.items():\n",
    "#     print(f\"Training {classifier_name}...\")\n",
    "    \n",
    "#     # Train the classifier\n",
    "#     classifier.fit(x_train_tfidf, y_train)\n",
    "    \n",
    "#     # Predict on the test set\n",
    "#     y_pred = classifier.predict(x_test_tfidf)\n",
    "    \n",
    "#     # Generate a classification report\n",
    "#     report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "#     # Print accuracy and store the report for further analysis\n",
    "#     print(f\"{classifier_name} Accuracy: {report['accuracy']}\")\n",
    "#     evaluation_reports[classifier_name] = report\n",
    "    \n",
    "#     # Detailed report\n",
    "#     print(f\"{classifier_name} Classification Report:\")\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 1275]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[1;32m     24\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(x_test_tfidf)\n",
      "File \u001b[0;32m~/Documents/AIProjects/Automated-Software-Refactoring-Commit-Classifier/myenv/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AIProjects/Automated-Software-Refactoring-Commit-Classifier/myenv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[0;32m~/Documents/AIProjects/Automated-Software-Refactoring-Commit-Classifier/myenv/lib/python3.12/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/AIProjects/Automated-Software-Refactoring-Commit-Classifier/myenv/lib/python3.12/site-packages/sklearn/utils/validation.py:1281\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1264\u001b[0m     X,\n\u001b[1;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1277\u001b[0m )\n\u001b[1;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m-> 1281\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/Documents/AIProjects/Automated-Software-Refactoring-Commit-Classifier/myenv/lib/python3.12/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 1275]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "    \n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(bootstrap=False, criterion='gini', max_depth=None, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease= 0.0011111111111111113, min_samples_leaf= 1, min_samples_split= 3, n_estimators= 950),\n",
    "    \"Support Vector Classification\": SVC(gamma='auto', kernel='linear', C=0.8119),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=20, criterion='gini'),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(alpha=2.63),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=69, weights='uniform')\n",
    "}\n",
    "\n",
    "# RandomForestClassifier(max_depth=78, n_estimators=500, criterion='gini', bootstrap=False)\n",
    "\n",
    "evaluation_reports = {}\n",
    "\n",
    "# Train, predict, and evaluate each classifier separately\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    print(f\"Training {classifier_name}...\")\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(x_train_tfidf, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = classifier.predict(x_test_tfidf)\n",
    "    \n",
    "    # Generate a classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Store the report for further analysis\n",
    "    evaluation_reports[classifier_name] = report\n",
    "    \n",
    "    # Extract and print the weighted average F1-score for the classifier\n",
    "    f1_score = report['macro avg']['f1-score']\n",
    "    display(f\"{classifier_name} Weighted Average F1 Score: {f1_score:.4f}\")\n",
    "    \n",
    "    # Detailed report (optional, can be commented out if not needed)\n",
    "    print(f\"{classifier_name} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = ['internal', 'bugfix', 'external', 'functional', 'code smell']\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix with Class Names')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of x_train_tfidf: 1\n",
      "Length of y_train: 1275\n",
      "The number of samples in x_train_tfidf and y_train do not match!\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of x_train_tfidf:\", x_train_tfidf.shape[0])\n",
    "print(\"Length of y_train:\", len(y_train))\n",
    "\n",
    "if x_train_tfidf.shape[0] != len(y_train):\n",
    "    print(\"The number of samples in x_train_tfidf and y_train do not match!\")\n",
    "else:\n",
    "    print(\"The lengths are consistent. Ready to train the model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
